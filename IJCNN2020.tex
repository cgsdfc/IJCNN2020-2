
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{multirow}
\usepackage{euscript}
\usepackage{url}
\usepackage{pstricks, pst-node}
\usepackage{textcomp}
\usepackage{stmaryrd}
\usepackage{epsfig}
% \usepackage{subfigure}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tabularx}
\usepackage{epstopdf}


\usepackage{graphicx}
\usepackage{latexsym}
\usepackage[fleqn]{amsmath}
\usepackage[varg]{txfonts}
\usepackage{float}

\usepackage{graphicx, graphics}
\usepackage{algorithmic}
\usepackage{tabularx}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{subcaption}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}


\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}

\newenvironment{figurehere}
  {\def\@captype{figure}}
  {}
\makeatother




% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



% \usepackage{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{An Empirical Analysis for Automatic Evaluation of Generative Dialogue Systems}

\author{
\IEEEauthorblockN{Cong Feng$^{1,2}$, Wenge Rong$^{1,2}$, Shijie Zhou$^{1,2}$, Jianfei Zhang$^{1,2}$, Zhang Xiong$^{1,2}$}
\IEEEauthorblockA{
	$^{1}$State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China\\
	$^{2}$School of Computer Science and Engineering, Beihang University, Beijing 100191, China\\
\{congfeng, w.rong, zhoushijie, zhangjf, xiongz\}@buaa.edu.cn}
}

\maketitle

\begin{abstract}
Evaluating generative dialogue systems with automatic metrics is a challenging task.
It has been shown that the word-overlap metrics such as BLEU and the word-embedding metrics do not correlate well with human judgements.
Furthermore, regardless of the correlation with human judgements, the scores of these metrics do not correlate well with others, which means fine-tuning a dialogue system on one set of metrics may yield inconsistency when testing with another set of metrics.
This infeasibility again adds to the ineffectiveness of evaluation dialogue systems.
To the end of utilizing metrics at its best, in this research, we offer several pragmatic suggestions on the use of automatic metrics to avoid the issue of metric inconsistency by empirically evaluating a set of automatic evaluation metrics.
\end{abstract}

\begin{IEEEkeywords}
Automatic Metrics, Dialogue Response Generation, Chatbot
\end{IEEEkeywords}

\section{Introduction}
Recently the chat-oriented dialogue systems have seen a boom in the community.
These systems are trained to make an appropriate response given a conversational context and can be applied to various applications.
One of the fundamental techniques in such dialogue systems is generative model, which can learn language patterns and extract knowledge from the corpus in an unsupervised manner.
The research community has made steady progress with the generative models and one of the most widely used techniques is Seq2Seq model \cite{Seq2Seq}.
Though generative models have shown promising results, there exist one essential challenge.
They incline to give meaningless responses to the questions, e.g., \textit{I don't know}, thereby making their evaluation remain an open problem \cite{HowNot}.

Currently most generative dialogue systems are evaluated by borrowing automatic metrics from other tasks, e.g., machine translation, such as the BLEU \cite{BLEU}, METEOR \cite{METEOR} and etc.
However, the correlations between these borrowed metrics and human judgements were unclear and researchers generally fell back to human evaluation for better accuracy and reliability \cite{VHRED,Shang}.
It is found that those metrics only correlated weakly with human judgements on the non-technical Twitter corpus and not at all on the technical Ubuntu Dialogue Corpus \cite{HowNot}, which calls for new automatic metrics that are more relevant to human judgements.

In this paper, we followed the previous work and further investigated the behaviors of automatic metrics without human judgements.
We analyzed both system-level and example-level scores to find out possible correlations among different metrics.
In particular, we drew samples from the example-level scores of different metrics and analyze the pairwise correlation of these samples.
It is found that some pairs of samples have a high correlation, while other pairs show a much lower one.

Based on these observations, we further clustered the metrics based on the pairwise correlations of their corresponding samples.
The results show that similar metrics tend to cluster together, indicating a high pairwise correlation within the same group.
According to the experiment results, it is argued that the scores given by dissimilar metrics have the risk of high inconsistency, which is yet another pitfall of using automatic metrics.
Therefore, it is strongly recommended to choose consistent metrics for the ease of comparison across different settings.

The rest of the paper is organized as follows: Section II will present the related work in the literature about the evaluation metrics on dialogue systems.
Section III will illustrated the empirical study's setup and configuration and Section IV will present and discuss the result.
Section V will conclude the paper and point out potential future direction.

\section{Related Work}
In dialogue response generation, to address the issue of low metric-human correlation, various semantic-based methods have been proposed.
For example, the ADEM metric proposed models the human judgements with a feed-forward neural network \cite{ADEM}.
The RUBER metric takes an asymmetric approach w.r.t the response-context pair and response-reference pair, where the former is modeled by a neural network and the latter is measured by an embedding-based metric.
They combined two scores with various heuristics and achieved improvements on a Chinese corpus \cite{RUBER}.
In brief, correlation with human judgements has been the supervised signal guiding the evolution of automatic metrics.

Other popular metrics include perplexity, which is generally used to evaluate statistical language models.
Vinyals et al. revealed that their Seq2Seq dialogue model achieved a much lower perplexity than the n-gram baseline, but they also admitted the drawbacks of using such a metric \cite{GoogleChatbot}.
Serban et al. also used perplexity to evaluate their models \cite{HRED}, along with other metrics.
However they were also not clear about how well these metrics accounted for the grammatical correctness and semantic coherence of the responses.

Generally, automatic metrics have been constantly doubted of their capability to reflect human judgements.
In one of the earliest attempts to the response generation problem, Ritter et al. made an initial examination on the suitability of BLEU to this field leveraging the human data they collected \cite{Ritter11}.
They found that the BLEU scores were very low even on the system level and the correlation was modest.
Similarly, Shang et al. also argued that BLEU did not apply as the reasonable responses evaluation \cite{Shang}.

An extensive study of metric-human correlation was conducted by Liu et al. \cite{HowNot}.
It was revealed that although these metrics could distinguish state-of-art models from baseline models, none of them correlated highly with human scores.
Their work has left us two questions: 1) How can we improve the metric-human correlation? 2) Why do the existing metrics correlate badly with human judgements?
Previous works that proposed enhanced metrics endeavored to answer the first question, while we try to shed some light on the second one.
In particular, we are curious about what can we learn when scores from different metrics are put together and compared across various settings.


\section{Experiment Configurations}
Our experiment essentially involves training multiple generative models on multiple datasets, and then measure their performances with various automatic metrics.
As stated, our study does not involve human evaluation.
We also do not consider retrieval models as we mainly focuse on generative models.
Due to the time and resource constrains, we limit our scope to three commonly used models and three datasets.

% ------------------------- %
% --------- Metrics ------- %
% ------------------------- %
\subsection{Metrics}
Here are a list of popular metrics used in dialogue response evaluation included in this research.

1) BLEU \cite{BLEU} is a classical metric for machine translation that reports a high correlation with human scores on the system level.
It owes the quality of a hypothesis to its similarity to multiple references.
It computes the geometric mean of consecutive orders of n-gram precision, multiplied by a brevity penalty.

2) METEOR \cite{METEOR} is a metric proposed to address several issues with BLEU.
It applies multiple stages of unigram matching to the hypothesis and reference, each using a different criterion, such as exact matching, WordNet synonyms, and paraphrases.
An alignment is then created from these unigram matches.
The score is based on the F1 of the alignment and a penalty to shorter matches.

3) ROUGE \cite{ROUGE} is a family of metrics for automatic summarization.
It is based on the F1 score and can integrate different counting units, e.g., n-gram statistics and the longest common subsequence.

4) Embedding Average is a metric based on word embedding, a distributed approach to the meaning of words \cite{word2vec}.
The embedding of a sentence is defined as the average of the embeddings of its composing words.
The similarity of two sentences is then simply defined as the cosine of the corresponding vectors.

5) Vector Extrema \cite{Vector_Extrema} composes the sentence embedding by taking the most extreme value (either maximum or minimum) along each dimension from its constitutive words.
The intuition is that in the embedding space, common words like function words are pulled towards the origin as they appear in the context of many different words, while informative words are pushed away from the origin in either positive or negative direction, since they tend to appear in more specific context.

6) Greedy Matching \cite{GreedyAndOptimal} is an embedding-based method without calculating a sentence vector.
Instead, two sentences under comparison are treated as a weighted bipartite graph, taking their words as nodes and the embedding cosine of two words $\cos(w, w')$ as the edge weight.
The metric is based on a greedy method to solve the optimal matching problem on the weighted graph:
\begin{align}
    G(r, \hat{r}) = \frac{
    \sum_{w \in r} \max_{\hat{w} \in \hat{r}} \cos(e_w, e_{\hat{w}})
    }{ |r| } \\
    \textit{Greedy-Matching} = \frac{
    G(r, \hat{r}) + G(\hat{r}, r)
    }{2}
\end{align}
where $r$ and $\hat{r}$ are the reference and response, respectively.

7) ADEM \cite{ADEM} is an approach based on a concise nueral network.
The model first embeds a context $c$, response $\hat{r}$, and reference $r$ into a low-rank vector space and then predicts the human score with a linear combination of the input features:
\begin{align}
    s(c, r, \hat{r}) = \frac{(c^T M \hat{r} + r^T N \hat{r} - \alpha)}{\beta}
\end{align}
$M, N$ are learnable parameters of the network and $\alpha, \beta$ are constants to normalize the output to a five-point score.
The model is trained on a human-annotated conversation corpus to minimize the squared error with L2 regularization:
\begin{align}
    L = \sum_{i=1}^{K} (s_i - h_i)^2 + \gamma \left\| \theta \right\| _2
\end{align}
where $K$ is the number of samples in a batch.
$s_i$ and $h_i$ are the model prediction and the human score, respectively.
$\theta$ is the parameters of the network and $\gamma$ is the regularization factor.

8) Distinct-N \cite{MMI} measures the rate of unique n-grams in a sentence.
For a sentence, it is the number of unique n-grams divided by the total number of n-grams.
It is a token-level measurement of the diversity of a response.

% ------------------------- %
% --------- Models -------- %
% ------------------------- %
\subsection{Models}
In our research, three representative models from \cite{VHRED} are selected, namely LSTM, HRED, and VHRED. LSTM is a simple model based on the Long Short-Term Memory \cite{LSTM}.
HRED extends the standard Seq2Seq framework by using a hierarchical encoder.
VHRED is an extension to HRED that injects randomness into the decoder to achieve higher diversity.
As such, they form a hierarchy of architectural sophistication and are ideal baselines of generative models.

1) The LSTM model is a simple generative model with a single RNN acting as both an encoder and a decoder.
Note that the generative LSTM model is not autoregressive as the output of the previous time step does not become the input of the current time step.

2) The HRED model \cite{hred-qs,HRED} features a hierarchical encoding mechanism that takes into account the structure of dialogues.
It first encodes each sentence with the \emph{utterance encoder} into a fixed-length utterance vector $e_u$. Then, the utterance vectors are processed iteratively by the \emph{context encoder} to produce a fixed-length context vector $e_d$.
Finally, the \emph{utterance decoder} takes the context vector and generates the next utterance of the dialogue.

3) The VHRED model \cite{VHRED} extends the HRED model with a variational inference mechanism.
Essentially, VHRED injects into the utterance decoder random variables that are sampled from a high-dimension normal distribution, the parameters of which are conditioned on the context vector $e_d$.

The configurations for different model components are shown in Tables 1-4 .
We used Adam \cite{AdamOpt} as our optimizer and applied gradient clipping with a threshold of 1.
The learning rate was set to 0.0002 on the Ubuntu corpus and 0.0002 on the others.
All the models were trained on a Nvidia GTX for at least one week.
We used random sampling when testing.
\input{data/model_config.tex}

% --------------------------- %
% --------- Datasets -------- %
% --------------------------- %
\subsection{Datasets}
In this research, we select three commonly used datasets, the statistics of which are listed in Tables 5-6.
These datasets represent three common domains in the literature, namely technical support, movie subtitles and online forums.
\input{data/dataset_stats.tex}

1) The Ubuntu Dialogue Corpus \cite{ubuntu_corpus} is a large-scale multi-turn dyadic technical corpus collected from the Ubuntu channel of an IRC network\footnote{\url{https://irclogs.ubuntu.com/}}.
It contains a lot of technical symbols, such as filesystem paths, commands, and URLs.

2) The OpenSubtitles dataset \cite{opensub} is an enormous corpus of movie subtitles.
It is collected by the OPUS project \cite{OPUS} from the opensubtitles website\footnote{\url{http://www.opensubtitles.org}}.
Typically, two consecutive utterances are treated as a context-response pair, as in \cite{GoogleChatbot,MMI}.

3) LSDSCC \cite{LSDSCC} is a domain-specific conversation corpus collected from the movie subreddit of the Reddit forum\footnote{\url{https://www.reddit.com/r/datasets}}.
It is believed that a more specific domain can help avoid generating universal responses \cite{LSDSCC}.


\section{Experimental Result and Discussion}
\subsection{System-Level Scores}
We first measured the system-level scores for all settings, shown in Table 7.
The system-level score reflects the average performance of a model trained on a dataset.
For metrics that do not have an explicit system-level definition, we took the arithmetic average of their example-level scores.
\input{data/system_level_scores.tex}

The system-level scores of different metrics look quite consistent in terms of the best performing model on a certain dataset.
On LSDSCC, for example, the HRED model beats all the other models on all but one metrics, while on OpenSubtitles, the VHRED model wins the best of most of the metrics.
The system-level score seems to be able to distinguish state-of-the-art models from baselines, since most of the metrics agree on which model is the best.

However, the consistency among the metrics does not necessarily lead to a high correlation with human judgement, as shown in \cite{HowNot}.
Moreover, since the system-level score is calculated by accumulating over the example-level scores, it is possible that the inconsistency on the example-level is hidden away.
To reveal the possible inconsistency, we performed analyses on the example-level scores.

\subsection{Example-Level Scores}
On the example level, a matrix $M \in R^{N \times M}$ is calculated for each model instance, where $N$ is the number of examples and $M$ is the number of metrics.
Each row $r_i$ of the matrix is the scores of all metrics for an example $e_i$, while each column $c_j$ is the values of a metric $s_j$ computed for all examples.
All the metrics share the same set of examples within a matrix.
Thus, the correlation of any two columns $c_i, c_j$ can be understood as to how much the corresponding metrics $s_i, s_j$ agree on their shared examples.
We compute the pairwise correlations of the metrics and highlight the degree of correlation with heatmaps, as shown in Fig. \ref{fig:corr_heatmap}.
We used Pearson's r and all the results are statistically significant.

\input{data/heatmap.tex}

Each subfigure in Fig. \ref{fig:corr_heatmap} was plotted from the correlation matrix for a model instance.
The color of the cells represents the degree of correlation between the row and column labels, with red, blue, and white stand for positive, negative and zero correlation, respectively.
One can observe red regions divided by white or blue lines from these plots, showing the signs of clustering.

To better observe the agreement and disagreement among the metrics, we applied hierarchical clustering to the metrics based on their correlations and the results are shown in Fig. \ref{fig:hierarchy}.
A hierarchical clustering algorithm starts with a forest of nodes and iteratively merges them into larger clusters until the root cluster is created.
We used the following node-level distance $\textit{dist}(\cdot, \cdot)$ and cluster-level distance $d(\cdot, \cdot)$:
\begin{align}
    \textit{dist}(i, j) &= 1 - \textit{corr}(i, j) \\
    d(u, v) &= \frac{\sum_{i,j}\textit{dist}(i, j)}{|u| \cdot |v|}
\end{align}
where $i$ and $j$ are points in cluster $u$ and $v$, respectively. $|u|$ and $|v|$ are the cardinalities of cluster $u$ and $v$, respectively\footnote{This is also known as the average method.}. Again the correlation is based on Pearson's r.
\input{data/hierarchy.tex}

From Fig. \ref{fig:hierarchy}, we observe a hierarchy of agreement among different metrics.
The clusters created in the earlier iterations appear closer to the leaves of the tree, which indicates higher pairwise correlations or stronger inter-metric agreement.
As a cluster grows larger, its children become more loosely connected with greater distances.
It is interesting to find that the dendrograms in Fig. \ref{fig:hierarchy} share some regular structure, which has an observable correspondence to the category of the metrics. Across all the settings, we generally observe these independent clusters, named after the shared category or representative element:

1) Word-Overlap: a large cluster that contains many subclusters formed by different word-overlap metrics, such as BLEU, ROUGE, and METEOR.

2) Word-Embedding: a small cluster that contains Vector-Average, Vector-Extrema, and Greedy-Matching.

3) ADEM: a standalone cluster formed by ADEM.

4) Distinct-1: a metric that might belong to some cluster or become a standalone cluster depending on the dataset involved.

5) Distinct-2 and \#words: a generally observable cluster.

Those clusters confirm our hypothesis on the inconsistency among different metrics.
Luckily, there are still some forms of agreement that the same cluster of metrics can reach.
Unfortunately, the exact reasons why the clusters are formed this way are unclear to us.
Although our experiments have a simple procedure, it should be noted that the mechanism behind each step has a complex nature.
For example, the dialogue datasets are intrinsically very diverse and the way the generative models work is not well-understood.
Besides, it is also unclear how these metrics reflect the desirable properties of a response.
Hence we can only conclude that similar metrics tend to have consistent results on the example-level.
The similarity of metrics generally refers to their mechanisms, such as the way to extract overlap units or derive semantics from an utterance, or the way to combine different components.

Specifically, we find that metrics based on word overlap tend to have high pairwise correlations since they all make use of the n-gram statistics somehow.
We also find that ADEM does not correlate well with all the other metrics since it has a much higher correlation with human judgement while other metrics do not.

\subsection{Qualitative Analysis}
\input{data/example/OpenSubtitles.tex}
Table 8 serves as an example of the inter-metrics inconsistency.
The response from HRED mentions ``her room'', which is relevant to the subject ``she'' in the context and the verb ``starts'' matches the ``start'' in the reference.
The response from VHRED does not share any token with the context or reference, but it remarks the event mentioned by the context so it is quite a reasonable one.

All three responses share no n-grams with either the context or the reference in terms of exact matching.
Thus, all the word-overlap metrics yield a zero value.
However, all the word-embedding metrics give non-zero value, making them incompatible with the word-overlap metrics.

In terms of semantic relevance, the responses from both HRED and VHRED are somehow related to the topic of the dialogue, while the response from LSTM looks grammatically incomplete.
One will mostly agree that the responses from HRED and VHRED are equally better than that from LSTM. Nonetheless, the ranks given by the word-embedding metrics do not agree with our manual inspection.
For example, they all give higher scores to LSTM than HRED.


\section{Conclusion and Future Work}
In this paper, we followed the work of \cite{HowNot} and try to understand the reasons behind low metric-human correlations.
We investigated the system-level scores and leveraged statistical analyses to reveal the inter-metrics correlation on the example level.
Our study shows a high consistency of metrics on the system level, as shown by others \cite{HowNot,VHRED,GoogleChatbot}.
We further show that on the example level, similar metrics tend to score the examples more consistently and the degree of correlations forms a hierarchical cluster.

Intuitively, different metrics judge a dialogue from different angles, while human beings judge it from a quite comprehensive perspective.
This might explain why the metrics do not correlate well with human judgements.
Our discovery of the example-level correlation-based hierarchical clusterings of metrics is a novel contribution.

Based on the observations, it is recommended to avoid using a set of metrics that have low pairwise correlations since that will make the example-level scores divergent and hard to explain.
We also urge against the use of metrics that are known to have poor correlations with human judgements.
In the future, we would like to look deeper into the mechanisms behind the metric-human correlations.

\section*{Acknowledgments}
This work was partially supported by the State Key Laboratory of Software Development Environment of China (No. SKLSDE-2019ZX-15).

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% control reference type settings
\balance

\bibliographystyle{IEEEtran}
\bibliography{reference}

% that's all folks
\end{document}
